{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Cross Validation and Gradient Descent\n",
    "## by Tiffany Nguyen\n",
    "The purpose of this lab is to conduct cross validtion for selecting an optimal lambda for ridge regression and implement gradient descent from scratch using just the NumPy library\n",
    "### Part 1: Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainDataX shape: (1595, 96)\n",
      "trainDataY shape: (1595,)\n"
     ]
    }
   ],
   "source": [
    "# Read all training data\n",
    "trainDataX = np.empty((0, 95))\n",
    "trainDataY = np.empty((0))\n",
    "\n",
    "#read train data\n",
    "with open(\"crime-train.txt\", \"r\") as filestream:\n",
    "  next(filestream) #skip header line\n",
    "  for line in filestream:\n",
    "    currentline = line.strip().split(\"\\t\")\n",
    "    trainDataX = np.append(trainDataX, [np.array(currentline[1:], dtype=float)], axis=0)\n",
    "    trainDataY = np.append(trainDataY, float(currentline[0]))\n",
    "\n",
    "# add dummy feature to train data (column of ones for bias) \n",
    "dummyCol = np.ones((trainDataX.shape[0], 1))\n",
    "trainDataX = np.concatenate((trainDataX, dummyCol), axis=1)\n",
    "\n",
    "# print output\n",
    "print(\"trainDataX shape:\", trainDataX.shape)\n",
    "print(\"trainDataY shape:\", trainDataY.shape)\n",
    "# print(\"trainDataX: \\n\", trainDataX)\n",
    "# print(\"trainDataY: \\n\", trainDataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testDataX shape: (399, 96)\n",
      "testDataY shape: (399,)\n"
     ]
    }
   ],
   "source": [
    "# Read all testing data\n",
    "testDataX = np.empty((0, 95))\n",
    "testDataY = np.empty((0))\n",
    "\n",
    "#read train data\n",
    "with open(\"crime-test.txt\", \"r\") as filestream:\n",
    "  next(filestream) #skip header line\n",
    "  for line in filestream:\n",
    "    currentline = line.strip().split(\"\\t\")\n",
    "    testDataX = np.append(testDataX, [np.array(currentline[1:], dtype=float)], axis=0)\n",
    "    testDataY = np.append(testDataY, float(currentline[0]))\n",
    "\n",
    "# add dummy feature to train data (column of ones for bias) \n",
    "dummyCol = np.ones((testDataX.shape[0], 1))\n",
    "testDataX = np.concatenate((testDataX, dummyCol), axis=1)\n",
    "\n",
    "# print output\n",
    "print(\"testDataX shape:\", testDataX.shape)\n",
    "print(\"testDataY shape:\", testDataY.shape)\n",
    "# print(\"testDataX: \\n\", testDataX)\n",
    "# print(\"testDataY: \\n\", testDataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: K-fold Cross Validation\n",
    "**Selected Lambda**: My selected Lambda is 25, and its RMSE is 0.13591585919851965. I selected 25 becuase, after performing k-fold cross validation and getting the average RMSE of all folds for each lambda, the average RMSE was smallest when lambda was 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numPerSet: 319\n"
     ]
    }
   ],
   "source": [
    "# Divide data into k sets\n",
    "k = 5\n",
    "numPerSet = int(trainDataX.shape[0] / k)\n",
    "print(\"numPerSet:\", numPerSet)\n",
    "\n",
    "fold1X = trainDataX[0:numPerSet]\n",
    "fold2X = trainDataX[numPerSet:2*numPerSet]\n",
    "fold3X = trainDataX[2*numPerSet:3*numPerSet]\n",
    "fold4X = trainDataX[3*numPerSet:4*numPerSet]\n",
    "fold5X = trainDataX[4*numPerSet:5*numPerSet]\n",
    "allFolds = np.array([fold1X, fold2X, fold3X, fold4X, fold5X])\n",
    "# print(allFolds[0].shape)\n",
    "\n",
    "fold1Y = trainDataY[0:numPerSet]\n",
    "fold2Y = trainDataY[numPerSet:2*numPerSet]\n",
    "fold3Y = trainDataY[2*numPerSet:3*numPerSet]\n",
    "fold4Y = trainDataY[3*numPerSet:4*numPerSet]\n",
    "fold5Y = trainDataY[4*numPerSet:5*numPerSet]\n",
    "allFoldsY = np.array([fold1Y, fold2Y, fold3Y, fold4Y, fold5Y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Lambda Values: [400. 200. 100.  50.  25.  12.   6.   3.   1.   0.]\n"
     ]
    }
   ],
   "source": [
    "# Create list to store lambda and RMSE values\n",
    "lambdaVal = 400 #initial lambda value\n",
    "allLambda = np.empty((10))\n",
    "for i in range(10):\n",
    "  allLambda[i] = int(lambdaVal)\n",
    "  lambdaVal /= 2\n",
    "print(\"All Lambda Values:\", allLambda)\n",
    "allRMSE = np.empty((10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All RMSE: [0.14951278 0.14069402 0.13727677 0.13615594 0.13591586 0.13603416\n",
      " 0.1362846  0.13658964 0.13707909 0.13777474]\n",
      "Min RMSE: 0.13591585919851965\n",
      "Optimal Lambda 25.0\n"
     ]
    }
   ],
   "source": [
    "# Perform k-fold cross test validation\n",
    "for i in range(len(allLambda)):\n",
    "  curRMSE = np.empty((k)) # all RMSE associated with current lambda value for each k fold \"set\"\n",
    "  \n",
    "  # Calculate RMSE all fold combinations\n",
    "  for curK in range(k):\n",
    "    # get train and validation data from folds\n",
    "    initialIndex = 0 if curK != 0 else 1\n",
    "    curTrainX = allFolds[initialIndex]\n",
    "    curTrainY = allFoldsY[initialIndex]\n",
    "    for curFoldIndex in range(len(allFolds)):\n",
    "        if(curFoldIndex != curK and curFoldIndex != initialIndex):\n",
    "          curTrainX = np.concatenate((curTrainX, allFolds[curFoldIndex]), axis=0)\n",
    "          curTrainY = np.concatenate((curTrainY, allFoldsY[curFoldIndex]), axis=0)\n",
    "    curValidX = allFolds[curK]\n",
    "    curValidY = allFoldsY[curK]\n",
    "    # print(curValid.shape) # should be (319, 96)\n",
    "    # print(curTrainX.shape) # should be (1276, 96)\n",
    "\n",
    "    # calculate ridge regression weight\n",
    "    xtx = np.dot(np.transpose(curTrainX), curTrainX)\n",
    "    lambdaIdentity = allLambda[i] * np.identity(curTrainX.shape[1])\n",
    "    xtxLambdaInv = np.linalg.inv(xtx + lambdaIdentity)\n",
    "    xtxLambdaInvXt = np.dot(xtxLambdaInv, np.transpose(curTrainX))\n",
    "    ridgeRegressionWeights = np.dot(xtxLambdaInvXt, curTrainY)\n",
    "\n",
    "    # get ridge regression prediction using validation data\n",
    "    ridgePredValid = np.dot(ridgeRegressionWeights, np.transpose(curValidX))\n",
    "\n",
    "    # evaluate RMSE for current ridge regression\n",
    "    curRMSE[curK] = np.sqrt(np.sum(np.square(ridgePredValid - curValidY))/len(ridgePredValid))\n",
    "\n",
    "  # get average RMSE for the given fold\n",
    "  allRMSE[i] = np.average(curRMSE)\n",
    "\n",
    "# find optimal lambda using min RMSE\n",
    "print(\"All RMSE:\", allRMSE)\n",
    "print(\"Min RMSE:\", np.min(allRMSE))\n",
    "optimalLambda = allLambda[np.argmin(allRMSE)]\n",
    "print(\"Optimal Lambda\", optimalLambda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Gradient Descent\n",
    "Equation to update the weights:\n",
    "$$ w_{t+1} = w_t + \\alpha \\Delta L(w_t)$$\n",
    "For Linear Regression, this becomes\n",
    "$$ w_{t+1} = w_t + \\alpha  x^T(y-xw^T)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weight: -1.9712730245587566\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "initialWeight = np.random.normal()\n",
    "alpha = 0.00001\n",
    "convergeMax = 10**-5\n",
    "print(\"initial weight:\", initialWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2697546239876076\n",
      "curRMSE 0.13688241913867798\n",
      "weights [ 1.74107285e-01  2.48577605e-02 -2.25011068e-02 -6.70545547e-02\n",
      "  1.41280495e-02 -1.82205037e-03 -2.03715959e-01  3.92210862e-02\n",
      " -3.94820146e-02 -4.40020252e-02  6.20517485e-03 -4.20224488e-02\n",
      " -2.75008292e-03 -3.50988836e-03 -7.79593411e-03  5.35526601e-03\n",
      "  1.55149196e-01 -1.47382538e-01 -1.77154372e-03 -2.68049761e-03\n",
      "  3.64686951e-03  1.14245320e-02  1.93869076e-02 -5.35700751e-02\n",
      "  5.07096933e-03 -2.34679453e-02  1.97118592e-02  2.60153974e-03\n",
      "  3.91621197e-02 -1.31035942e-02 -1.20558416e-03  2.67522038e-02\n",
      "  7.90170457e-03 -4.35640562e-01  6.12077382e-02 -5.75971870e-01\n",
      "  9.77420097e-01  2.75798483e-02  1.22277120e-01 -2.10792588e-01\n",
      " -1.11361923e-02 -1.11646924e-03  3.05582499e-03 -2.20938416e-02\n",
      " -3.53119880e-03  5.24184760e-02 -2.36766026e-02  1.01922078e-03\n",
      " -1.80181373e-02  5.30366972e-02 -3.59065175e-02 -2.00009456e-02\n",
      "  1.45576588e-01 -4.04243753e-01  3.22160090e-01 -2.80986941e-02\n",
      " -8.81770838e-02  1.46863809e-01 -1.83817052e-01  1.09981457e-01\n",
      " -6.85949913e-02 -1.42753479e-02 -7.10815982e-03  4.40128334e-02\n",
      "  9.79311823e-03  1.65386093e-03  4.13411683e-02 -6.11782672e-03\n",
      " -1.48408776e-02  1.07028906e-02 -1.21123495e-02  1.27632277e-02\n",
      "  4.68899562e-03  4.22566031e-03  1.18806132e-01 -2.29527453e-01\n",
      "  1.17114078e-01 -4.24246535e-02  1.57811309e-02 -2.46774754e-02\n",
      "  3.58394200e-02  1.51826147e-03 -1.11950117e-02 -1.67026327e-02\n",
      "  4.99609165e-03  2.78561889e-02  9.67887611e-03 -6.38301848e-03\n",
      "  7.89230484e-05 -9.32459873e-03  9.39400527e-03 -3.11711660e-03\n",
      " -7.10873915e-03 -1.09293742e-02  7.61733177e-03  2.36882101e-01]\n"
     ]
    }
   ],
   "source": [
    "# Perform gradient descent with linear regression\n",
    "def problem2(samplesX, samplesY):\n",
    "  np.random.seed(0)\n",
    "  curWeight = np.random.normal(0, 1, samplesX[0].shape)\n",
    "  prevWeight = -1 * np.ones(samplesX[0].shape)\n",
    "\n",
    "  print(max(curWeight))\n",
    "\n",
    "  while(max(np.subtract(curWeight, prevWeight)) > convergeMax):\n",
    "    prevWeight = curWeight\n",
    "    update = alpha * (np.dot(samplesX.T, np.subtract(samplesY, np.dot(samplesX, prevWeight.T))))\n",
    "    curWeight = prevWeight + update\n",
    "\n",
    "    # get and print current RMSE\n",
    "    predTrain = np.dot(curWeight, np.transpose(samplesX))\n",
    "    curRMSE =  np.sqrt(np.sum(np.square(predTrain - samplesY))/len(predTrain))\n",
    "    print(\"curRMSE\", curRMSE, end=\"\\r\")\n",
    "    \n",
    "  return(curWeight)\n",
    "\n",
    "gradDescWeights = problem2(trainDataX, trainDataY)\n",
    "print(\"\\nweights\", gradDescWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE 0.1368824191386779\n",
      "Test RMSE 0.1551456595144984\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using training dataset\n",
    "predTrain = np.dot(gradDescWeights, np.transpose(trainDataX))\n",
    "trainRMSE =  np.sqrt(np.sum(np.square(predTrain - trainDataY))/len(predTrain))\n",
    "print(\"Train RMSE\", trainRMSE)\n",
    "\n",
    "# Evaluate using testing dataset\n",
    "predTest = np.dot(gradDescWeights, np.transpose(testDataX))\n",
    "testRMSE =  np.sqrt(np.sum(np.square(predTest - testDataY))/len(predTest))\n",
    "print(\"Test RMSE\", testRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Ridge Regression with Cross Validation and Graident Descent\n",
    "Weight Update Equation for Ridge Regression:\n",
    "$$w_{t+1} = w_t + \\alpha[X^T(y-Xw^T)-\\lambda w^T]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 400.0\n"
     ]
    }
   ],
   "source": [
    "def problem3():\n",
    "  k = 5\n",
    "  # Perform k-fold cross test validation\n",
    "  for i in range(len(allLambda)):\n",
    "    print(\"Lambda\", allLambda[i])\n",
    "    curK_RMSE = np.empty((k)) # all RMSE associated with current lambda value for each k fold \"set\"\n",
    "    # Calculate RMSE all fold combinations\n",
    "    for curK in range(k):\n",
    "      print(\"current k\", curK)\n",
    "      # get train and validation data from folds\n",
    "      initialIndex = 0 if curK != 0 else 1\n",
    "      curTrainX = allFolds[initialIndex]\n",
    "      curTrainY = allFoldsY[initialIndex]\n",
    "      for curFoldIndex in range(len(allFolds)):\n",
    "          if(curFoldIndex != curK and curFoldIndex != initialIndex):\n",
    "            curTrainX = np.concatenate((curTrainX, allFolds[curFoldIndex]), axis=0)\n",
    "            curTrainY = np.concatenate((curTrainY, allFoldsY[curFoldIndex]), axis=0)\n",
    "      curValidX = allFolds[curK]\n",
    "      curValidY = allFoldsY[curK]\n",
    "\n",
    "      # calculate ridge regression weight using graident descent\n",
    "      curWeight = np.random.normal(0, 1, curTrainX[0].shape)\n",
    "      prevWeight = -1 * np.ones(curTrainX[0].shape)\n",
    "      while(max(np.subtract(curWeight, prevWeight)) > convergeMax):\n",
    "        prevWeight = curWeight\n",
    "        update = alpha * np.subtract(np.dot(curTrainX.T, np.subtract(curTrainY, np.dot(curTrainX, prevWeight.T))), np.dot(allLambda[i], prevWeight.T))\n",
    "        curWeight = prevWeight + update\n",
    "\n",
    "        # get and print current RMSE\n",
    "        predTrain = np.dot(curWeight, np.transpose(curTrainX))\n",
    "        curRMSE =  np.sqrt(np.sum(np.square(predTrain - curTrainY))/len(predTrain))\n",
    "        print(\"curRMSE\", curRMSE, end=\"\\r\")\n",
    "\n",
    "      # get ridge regression prediction using validation data\n",
    "      ridgePredValid = np.dot(curWeight, np.transpose(curValidX))\n",
    "\n",
    "      # evaluate RMSE for current ridge regression\n",
    "      curK_RMSE[int(curK)] = np.sqrt(np.sum(np.square(ridgePredValid - curValidY))/len(ridgePredValid))\n",
    "      print(\"\\n\") #get rid of \\r end\n",
    "\n",
    "    # get average RMSE for the given fold\n",
    "    allRMSE[i] = np.average(curK_RMSE)\n",
    "\n",
    "  # find optimal lambda using min RMSE\n",
    "  print(\"All RMSE:\", allRMSE)\n",
    "  print(\"Min RMSE:\", np.min(allRMSE))\n",
    "  optimalLambda = allLambda[np.argmin(allRMSE)]\n",
    "  print(\"Optimal Lambda\", optimalLambda)\n",
    "  return(optimalLambda)\n",
    "optimalLambda = problem3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coen140L",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
