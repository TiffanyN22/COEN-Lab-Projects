{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Spam Classification with Logistic Regression From Scratch\n",
    "## by Tiffany Nguyen\n",
    "### Part 1: Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainDataX shape: (3065, 57)\n",
      "trainDataY shape: (3065,)\n",
      "trainDataX: \n",
      " [[  0.      0.      0.    ...   6.718  33.    215.   ]\n",
      " [  0.      0.      0.    ...   2.044  22.     92.   ]\n",
      " [  0.      0.53    0.    ...   4.555  51.    123.   ]\n",
      " ...\n",
      " [  0.      0.      0.    ...   2.307   8.     30.   ]\n",
      " [  0.33    0.      0.    ...   1.271   5.     75.   ]\n",
      " [  0.31    0.      0.62  ...   1.142   3.     88.   ]]\n",
      "trainDataY: \n",
      " [1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Read all training data\n",
    "trainDataX = np.empty((0, 57))\n",
    "trainDataY = np.empty((0))\n",
    "\n",
    "#read train data\n",
    "with open(\"spambase 2/spam-train\", \"r\") as filestream:\n",
    "  for line in filestream:\n",
    "    currentline = line.strip().split(\",\")\n",
    "    trainDataX = np.append(trainDataX, [np.array(currentline[:-1], dtype=float)], axis=0)\n",
    "    trainDataY = np.append(trainDataY, float(currentline[-1])) # last column is label\n",
    "\n",
    "# print output\n",
    "print(\"trainDataX shape:\", trainDataX.shape)\n",
    "print(\"trainDataY shape:\", trainDataY.shape)\n",
    "print(\"trainDataX: \\n\", trainDataX)\n",
    "print(\"trainDataY: \\n\", trainDataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testDataX shape: (1536, 57)\n",
      "testDataY shape: (1536,)\n",
      "testDataX: \n",
      " [[0.000e+00 1.428e+01 0.000e+00 ... 1.800e+00 5.000e+00 9.000e+00]\n",
      " [5.100e-01 4.300e-01 2.900e-01 ... 6.590e+00 7.390e+02 2.333e+03]\n",
      " [7.900e-01 1.900e-01 9.000e-02 ... 2.324e+00 1.900e+01 3.650e+02]\n",
      " ...\n",
      " [0.000e+00 0.000e+00 7.600e-01 ... 2.441e+00 1.900e+01 2.490e+02]\n",
      " [0.000e+00 0.000e+00 8.700e-01 ... 1.601e+00 1.100e+01 2.770e+02]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 1.103e+00 3.000e+00 3.200e+01]]\n",
      "testDataY: \n",
      " [0. 1. 1. ... 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Read all testing data\n",
    "testDataX = np.empty((0, 57))\n",
    "testDataY = np.empty((0))\n",
    "\n",
    "#read test data\n",
    "with open(\"spambase 2/spam-test\", \"r\") as filestream:\n",
    "  for line in filestream:\n",
    "    currentline = line.strip().split(\",\")\n",
    "    testDataX = np.append(testDataX, [np.array(currentline[:-1], dtype=float)], axis=0)\n",
    "    testDataY = np.append(testDataY, float(currentline[-1])) # last column is label\n",
    "\n",
    "# print output\n",
    "print(\"testDataX shape:\", testDataX.shape)\n",
    "print(\"testDataY shape:\", testDataY.shape)\n",
    "print(\"testDataX: \\n\", testDataX)\n",
    "print(\"testDataY: \\n\", testDataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Perform Z-score Normalization\n",
    "Normalization is important so that features with larger scales don't have a larger influence on the model.\n",
    "\n",
    "Z-score normalization equation: $$Z=\\frac{X-\\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [1.07367047e-01 2.08107667e-01 2.88600326e-01 6.29396411e-02\n",
      " 3.15771615e-01 9.48123980e-02 1.15670473e-01 1.02619902e-01\n",
      " 8.88026101e-02 2.44913540e-01 5.70048940e-02 5.53249592e-01\n",
      " 9.29885808e-02 6.11941272e-02 4.60391517e-02 2.42646003e-01\n",
      " 1.41954323e-01 1.86929853e-01 1.65356607e+00 8.11680261e-02\n",
      " 7.91569331e-01 1.24639478e-01 1.00597064e-01 8.94681892e-02\n",
      " 5.21073409e-01 2.56143556e-01 7.60407830e-01 1.20991843e-01\n",
      " 9.69168026e-02 1.00707993e-01 6.22871126e-02 4.35986949e-02\n",
      " 9.31223491e-02 4.47373573e-02 1.00893964e-01 9.76835237e-02\n",
      " 1.31781403e-01 1.04567700e-02 7.94388254e-02 6.31908646e-02\n",
      " 4.73376835e-02 1.42339315e-01 4.48091354e-02 7.67993475e-02\n",
      " 2.99207178e-01 1.79725938e-01 4.91027732e-03 3.11060359e-02\n",
      " 3.88548124e-02 1.36330506e-01 1.69128874e-02 2.55486460e-01\n",
      " 8.03073409e-02 4.19836868e-02 4.97611909e+00 4.96247961e+01\n",
      " 2.88137684e+02]\n",
      "standard deviation: [3.14626401e-01 1.23933314e+00 5.15694908e-01 1.33776681e+00\n",
      " 7.00478932e-01 2.64696272e-01 4.13395224e-01 3.73181578e-01\n",
      " 2.79032531e-01 6.19060437e-01 1.90180737e-01 8.76127479e-01\n",
      " 3.07379613e-01 3.54168295e-01 2.53045731e-01 7.28876371e-01\n",
      " 4.20730427e-01 5.55841472e-01 1.77068187e+00 5.27421695e-01\n",
      " 1.17437623e+00 1.05404778e+00 3.60827040e-01 3.63625138e-01\n",
      " 1.54488469e+00 8.84263367e-01 3.37208843e+00 5.20135565e-01\n",
      " 6.03405281e-01 4.32747648e-01 3.99024123e-01 3.01445353e-01\n",
      " 4.97111532e-01 3.02916661e-01 5.39661712e-01 3.88945432e-01\n",
      " 4.09095638e-01 2.01106369e-01 4.52002698e-01 3.29825696e-01\n",
      " 3.98060828e-01 8.41030698e-01 2.13642467e-01 6.20319894e-01\n",
      " 1.06746981e+00 9.12248887e-01 7.75043025e-02 2.71477349e-01\n",
      " 2.42052655e-01 2.19762399e-01 1.02868141e-01 5.68848601e-01\n",
      " 2.69837974e-01 4.21186280e-01 3.21992747e+01 1.28298907e+02\n",
      " 6.40067086e+02]\n"
     ]
    }
   ],
   "source": [
    "# get mean and standard deviation from training data\n",
    "mean = np.mean(trainDataX, axis=0)\n",
    "std = np.std(trainDataX, axis=0)\n",
    "print(\"mean:\", mean)\n",
    "print(\"standard deviation:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3412525  -0.16791907 -0.55963385 ...  0.0540969  -0.12957863\n",
      "  -0.11426565]\n",
      " [-0.3412525  -0.16791907 -0.55963385 ... -0.09106165 -0.21531591\n",
      "  -0.30643301]\n",
      " [-0.3412525   0.25973027 -0.55963385 ... -0.01307853  0.01071875\n",
      "  -0.25800059]\n",
      " ...\n",
      " [-0.3412525  -0.16791907 -0.55963385 ... -0.08289376 -0.32443609\n",
      "  -0.40329786]\n",
      " [ 0.70761052 -0.16791907 -0.55963385 ... -0.1150684  -0.34781899\n",
      "  -0.33299273]\n",
      " [ 0.64404307 -0.16791907  0.64262739 ... -0.1190747  -0.36340759\n",
      "  -0.31268235]]\n"
     ]
    }
   ],
   "source": [
    "# normalize training data\n",
    "normTrainDataX = (trainDataX - mean)/std\n",
    "print(normTrainDataX)\n",
    "\n",
    "# normalize testing data\n",
    "normTestDataX = (testDataX - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Add Bias Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3412525  -0.16791907 -0.55963385 ... -0.12957863 -0.11426565\n",
      "   1.        ]\n",
      " [-0.3412525  -0.16791907 -0.55963385 ... -0.21531591 -0.30643301\n",
      "   1.        ]\n",
      " [-0.3412525   0.25973027 -0.55963385 ...  0.01071875 -0.25800059\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.3412525  -0.16791907 -0.55963385 ... -0.32443609 -0.40329786\n",
      "   1.        ]\n",
      " [ 0.70761052 -0.16791907 -0.55963385 ... -0.34781899 -0.33299273\n",
      "   1.        ]\n",
      " [ 0.64404307 -0.16791907  0.64262739 ... -0.36340759 -0.31268235\n",
      "   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# add dummy feature to train data (column of ones for bias) \n",
    "dummyCol = np.ones((normTrainDataX.shape[0], 1))\n",
    "normTrainDataX = np.concatenate((normTrainDataX, dummyCol), axis=1)\n",
    "print(normTrainDataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dummy feature to test data (column of ones for bias) \n",
    "dummyCol = np.ones((normTestDataX.shape[0], 1))\n",
    "normTestDataX = np.concatenate((normTestDataX, dummyCol), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Build a Logistic Regresion Model with Gradient Descent\n",
    "Use logistic regression to perform binary classification\n",
    "\n",
    "Sigmoid function used: $$S(r)=\\frac{1}{1+e^{-r}}$$\n",
    "Logistic Regression Weight Update Equation: $$w_{t+1}=w_t + \\alpha[X^T(y-S(Xw_t))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curRMSE 0.24338316077347366\n",
      "weights [-0.06579796 -0.33618968  0.0318817   0.4706584   0.39684911  0.21873301\n",
      "  1.04933305  0.20536465  0.13813264  0.07674643  0.02043958 -0.10012022\n",
      " -0.05541302  0.0071      0.47112823  0.68383264  0.38888239  0.10084451\n",
      "  0.12777687  0.27918425  0.21825479  0.370822    1.14822769  0.83288873\n",
      " -1.53258254 -0.90819946 -1.05280648  0.28917305 -0.95683862 -0.19882206\n",
      " -0.11006165 -2.13318186 -0.39211884  0.07270701 -1.30845069  0.2222756\n",
      "  0.07423266  0.05218067 -0.32100014 -0.26928943 -0.47556772 -1.12525713\n",
      " -0.39885262 -0.59308406 -0.58353188 -0.75232584 -0.46067251 -0.55090784\n",
      " -0.34294318 -0.02327669 -0.09147705  0.53980336  1.18986075  0.63355707\n",
      "  0.8652439   0.96517597  0.28292326 -1.36175323]\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "alpha = 0.0001\n",
    "convergeMax = 10**-3\n",
    "\n",
    "# Perform gradient descent with logistic regression\n",
    "def problem2(samplesX, samplesY):\n",
    "  np.random.seed(22)\n",
    "  curWeight = np.random.normal(0, 1, samplesX[0].shape)\n",
    "  prevWeight = -1 * np.ones(samplesX[0].shape)\n",
    "\n",
    "  while(max(np.subtract(curWeight, prevWeight)) > convergeMax):\n",
    "    prevWeight = curWeight\n",
    "    sigmoid = 1/(1+np.exp(-1 * np.dot(samplesX, prevWeight)))\n",
    "    update = alpha * (np.dot(samplesX.T, np.subtract(samplesY, sigmoid)))\n",
    "    curWeight = prevWeight + update\n",
    "\n",
    "    # get and print current RMSE\n",
    "    predTrain = 1/(1+np.exp(-1 *np.dot(curWeight, np.transpose(samplesX))))\n",
    "    curRMSE =  np.sqrt(np.sum(np.square(predTrain - samplesY))/len(predTrain))\n",
    "    print(\"curRMSE\", curRMSE, end=\"\\r\")\n",
    "  return(curWeight)\n",
    "\n",
    "gradDescWeights = problem2(normTrainDataX, trainDataY)\n",
    "print(\"\\nweights\", gradDescWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying Training Data:\n",
      " Num correct: 2841\n",
      " Num incorrect: 224\n",
      " Accuracy: 0.92692\n",
      "Classifying Testing Data:\n",
      " Num correct: 1424\n",
      " Num incorrect: 112\n",
      " Accuracy: 0.92708\n"
     ]
    }
   ],
   "source": [
    "# Classify output\n",
    "def classify(samplesX, samplesY):\n",
    "  numCorrect = 0\n",
    "  numIncorrect = 0\n",
    "\n",
    "  # make prediction\n",
    "  sigmoidPred = 1/(1+np.exp(-1 * np.dot(samplesX, gradDescWeights)))\n",
    "  pred = [1 if curPred >= 0.5 else 0 for curPred in sigmoidPred]\n",
    "\n",
    "  # get and print accuracy\n",
    "  for i in range(len(pred)):\n",
    "    if(pred[i] == samplesY[i]):\n",
    "      numCorrect+=1\n",
    "    else:\n",
    "      numIncorrect+=1\n",
    "  print(\" Num correct:\", numCorrect)\n",
    "  print(\" Num incorrect:\", numIncorrect)\n",
    "  print(\" Accuracy:\", round(numCorrect/(numCorrect+numIncorrect), 5))\n",
    "\n",
    "print(\"Classifying Training Data:\")\n",
    "classify(normTrainDataX, trainDataY)\n",
    "print(\"Classifying Testing Data:\")\n",
    "classify(normTestDataX, testDataY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coen140L",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
